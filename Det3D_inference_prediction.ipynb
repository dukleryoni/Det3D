{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box\n",
    "from pyquaternion import Quaternion\n",
    "import itertools\n",
    "import inference_utils\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "Axis = Any\n",
    "\n",
    "def get_detection_box(record):\n",
    "    return Box(record['translation'], record['size'], Quaternion(record['rotation']),\n",
    "                   name=record['detection_name'], token=record['sample_token'])\n",
    "\n",
    "def get_box(record):\n",
    "    return Box(record['translation'], record['size'], Quaternion(record['rotation']),\n",
    "                   name=record['category_name'], token=record['sample_token'])\n",
    "\n",
    "\n",
    "\n",
    "def boxes_to_sensor(boxes, pose_record, cs_record):\n",
    "    \"\"\"\n",
    "    Map boxes from global coordinates to the vehicle's sensor coordinate system.\n",
    "    :param boxes: The boxes in global coordinates.\n",
    "    :param pose_record: The pose record of the vehicle at the current timestamp.\n",
    "    :param cs_record: The calibrated sensor record of the sensor.\n",
    "    :return: The transformed boxes.\n",
    "    \"\"\"\n",
    "    boxes_out = []\n",
    "    for box in boxes:\n",
    "        # Create Box instance.\n",
    "        box = Box(box.center, box.wlh, Quaternion(box.orientation))\n",
    "\n",
    "        # Move box to ego vehicle coord system.\n",
    "        box.translate(-np.array(pose_record['translation']))\n",
    "        box.rotate(Quaternion(pose_record['rotation']).inverse)\n",
    "\n",
    "        #  Move box to sensor coord system.\n",
    "        box.translate(-np.array(cs_record['translation']))\n",
    "        box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "\n",
    "        boxes_out.append(box)\n",
    "\n",
    "    return boxes_out\n",
    "\n",
    "\n",
    "\n",
    "def visualize_sample(nusc,\n",
    "                     sample_token: str,\n",
    "                     gt_boxes,\n",
    "                     pred_boxes,\n",
    "                     scores,\n",
    "                     nsweeps: int = 1,\n",
    "                     conf_th: float = 0.15,\n",
    "                     eval_range: float = 50,\n",
    "                     verbose: bool = True,\n",
    "                     savepath: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes a sample from BEV with annotations and detection results.\n",
    "    :param nusc: NuScenes object.\n",
    "    :param sample_token: The nuScenes sample token.\n",
    "    :param gt_boxes: Ground truth boxes grouped by sample.\n",
    "    :param pred_boxes: Prediction grouped by sample.\n",
    "    :param nsweeps: Number of sweeps used for lidar visualization.\n",
    "    :param conf_th: The confidence threshold used to filter negatives.\n",
    "    :param eval_range: Range in meters beyond which boxes are ignored.\n",
    "    :param verbose: Whether to print to stdout.\n",
    "    :param savepath: If given, saves the the rendering here instead of displaying.\n",
    "    \"\"\"\n",
    "    # Retrieve sensor & pose records.\n",
    "    sample_rec = nusc.get('sample', sample_token)\n",
    "    sd_record = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])\n",
    "    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])\n",
    "\n",
    "    # Get boxes.\n",
    "#     boxes_gt_global = gt_boxes[sample_token]\n",
    "#     boxes_est_global = pred_boxes[sample_token]\n",
    "    boxes_gt_global = gt_boxes\n",
    "    boxes_est_global = pred_boxes\n",
    "\n",
    "    # Map GT boxes to lidar.\n",
    "    boxes_gt = boxes_to_sensor(boxes_gt_global, pose_record, cs_record)\n",
    "\n",
    "    # Map EST boxes to lidar.\n",
    "    boxes_est = boxes_to_sensor(boxes_est_global, pose_record, cs_record)\n",
    "\n",
    "#     # Add scores to EST boxes.\n",
    "#     for box_est, box_est_global in zip(boxes_est, boxes_est_global):\n",
    "#         box_est.score = box_est_global.detection_score\n",
    "\n",
    "    # Get point cloud in lidar frame.\n",
    "    pc, _ = LidarPointCloud.from_file_multisweep(nusc, sample_rec, 'LIDAR_TOP', 'LIDAR_TOP', nsweeps=nsweeps)\n",
    "\n",
    "    # Init axes.\n",
    "    %matplotlib notebook\n",
    "    _, ax = plt.subplots(1, 1, figsize=(9, 9))\n",
    "\n",
    "    # Show point cloud.\n",
    "    points = view_points(pc.points[:3, :], np.eye(4), normalize=False)\n",
    "    dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))\n",
    "    colors = np.minimum(1, dists / eval_range)\n",
    "    ax.scatter(points[0, :], points[1, :], c=colors, s=4)\n",
    "\n",
    "    # Show ego vehicle.\n",
    "    ax.plot(0, 0, 'x', color='black')\n",
    "\n",
    "    # Show GT boxes.\n",
    "    for box in boxes_gt:\n",
    "        box.render(ax, view=np.eye(4), colors=('g', 'g', 'g'), linewidth=2)\n",
    "\n",
    "    # Show EST boxes.\n",
    "    for (box,score) in zip(boxes_est,scores):\n",
    "        # Show only predictions with a high score.\n",
    "#         assert not np.isnan(box.score), 'Error: Box score cannot be NaN!'\n",
    "        if score >= conf_th:\n",
    "            box.render(ax, view=np.eye(4), colors=('b', 'b', 'b'), linewidth=1)\n",
    "\n",
    "    # Limit visible range.\n",
    "    axes_limit = eval_range + 3  # Slightly bigger to include boxes that extend beyond the range.\n",
    "    ax.set_xlim(-axes_limit, axes_limit)\n",
    "    ax.set_ylim(-axes_limit, axes_limit)\n",
    "    # Show / save plot.\n",
    "    if verbose:\n",
    "        print('Rendering sample token %s' % sample_token)\n",
    "    plt.title(sample_token)\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def visualize_prediction(nusc, model_pred, index=0,conf_th=0.15):\n",
    "    nusc_annos = inference_utils.get_nusc_style(model_pred, nusc) # convert into sample annos style object\n",
    "    sample_token, det_annotations_attrs = next(itertools.islice(nusc_annos['results'].items(), index, None)) # take a single frame\n",
    "    scores = model_pred[index]['scores']\n",
    "    \n",
    "    det_boxes = [get_detection_box(det_annotation_attr) for det_annotation_attr in det_annotations_attrs] #convert annos-style object into Box object\n",
    "    annotations_list =nusc.get('sample',sample_token)['anns']\n",
    "    gt_boxes = [get_box(nusc.get('sample_annotation', anno)) for anno in annotations_list]\n",
    "    \n",
    "    visualize_sample(nusc, sample_token, gt_boxes=gt_boxes, pred_boxes=det_boxes, scores=scores, conf_th=conf_th, nsweeps=10) # plot\n",
    "\n",
    "## visualizaing prediction from detection\n",
    "\n",
    "\n",
    "# visualize_sample(nusc, sample_token, gt_boxes=gt_boxes, pred_boxes=det_boxes, nsweeps=10)\n",
    "# visualize_prediction(nusc, predictions, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
